{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory Optimization for Retail - Data Preprocessing\n",
    "\n",
    "This notebook focuses on loading, cleaning, and preprocessing the retail sales and inventory data for our predictive analytics model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "First, we need to load our data from the CSV/Excel files. We'll check for the presence of these files in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List files in the data directory\n",
    "data_dir = '../data/'\n",
    "files = os.listdir(data_dir)\n",
    "print(f\"Files in data directory: {files}\")\n",
    "\n",
    "# Function to load data based on file extension\n",
    "def load_data(file_path):\n",
    "    if file_path.endswith('.csv'):\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_path.endswith(('.xlsx', '.xls')):\n",
    "        return pd.read_excel(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "Let's explore the dataset to understand its structure, check for missing values, and get a sense of the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the data (update file paths as needed)\n",
    "# Example: sales_data = load_data(os.path.join(data_dir, 'sales_data.csv'))\n",
    "\n",
    "# For demonstration, let's create a sample dataset\n",
    "# This will be replaced with actual data loading when files are available\n",
    "\n",
    "# Sample sales data\n",
    "dates = pd.date_range(start='2022-01-01', end='2023-12-31', freq='D')\n",
    "product_ids = [f'P{i:03d}' for i in range(1, 11)]\n",
    "store_ids = [f'S{i:02d}' for i in range(1, 6)]\n",
    "\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "sales_data = []\n",
    "\n",
    "for date in dates:\n",
    "    for product_id in product_ids:\n",
    "        for store_id in store_ids:\n",
    "            # Base sales with seasonal pattern\n",
    "            base_sales = 50 + 30 * np.sin(2 * np.pi * date.dayofyear / 365)\n",
    "            \n",
    "            # Add product-specific variation\n",
    "            product_factor = int(product_id[1:]) / 10\n",
    "            \n",
    "            # Add store-specific variation\n",
    "            store_factor = int(store_id[1:]) / 5\n",
    "            \n",
    "            # Add random noise\n",
    "            noise = np.random.normal(0, 10)\n",
    "            \n",
    "            # Calculate sales quantity\n",
    "            sales_qty = max(0, int(base_sales * product_factor * store_factor + noise))\n",
    "            \n",
    "            # Calculate inventory level (simple formula for demonstration)\n",
    "            inventory_level = max(0, int(sales_qty * 1.5 + np.random.normal(0, 20)))\n",
    "            \n",
    "            sales_data.append({\n",
    "                'Date': date,\n",
    "                'Product_ID': product_id,\n",
    "                'Store_ID': store_id,\n",
    "                'Sales_Quantity': sales_qty,\n",
    "                'Inventory_Level': inventory_level\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_sales = pd.DataFrame(sales_data)\n",
    "\n",
    "# Display the first few rows\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic data exploration\n",
    "print(f\"Dataset shape: {df_sales.shape}\")\n",
    "print(\"\\nData types:\")\n",
    "print(df_sales.dtypes)\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df_sales.describe())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_sales.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization\n",
    "\n",
    "Let's visualize the data to better understand patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Aggregate sales by date\n",
    "daily_sales = df_sales.groupby('Date')['Sales_Quantity'].sum().reset_index()\n",
    "\n",
    "# Plot daily sales\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_sales['Date'], daily_sales['Sales_Quantity'])\n",
    "plt.title('Daily Total Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales Quantity')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/daily_sales.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Aggregate sales by product\n",
    "product_sales = df_sales.groupby('Product_ID')['Sales_Quantity'].sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Plot product sales\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Product_ID', y='Sales_Quantity', data=product_sales)\n",
    "plt.title('Total Sales by Product')\n",
    "plt.xlabel('Product ID')\n",
    "plt.ylabel('Sales Quantity')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/product_sales.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Aggregate sales by store\n",
    "store_sales = df_sales.groupby('Store_ID')['Sales_Quantity'].sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Plot store sales\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Store_ID', y='Sales_Quantity', data=store_sales)\n",
    "plt.title('Total Sales by Store')\n",
    "plt.xlabel('Store ID')\n",
    "plt.ylabel('Sales Quantity')\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/store_sales.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sales vs Inventory scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Sales_Quantity', y='Inventory_Level', data=df_sales.sample(1000), alpha=0.6)\n",
    "plt.title('Sales Quantity vs Inventory Level')\n",
    "plt.xlabel('Sales Quantity')\n",
    "plt.ylabel('Inventory Level')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/sales_vs_inventory.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Now, let's preprocess the data for our time series forecasting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ensure Date is in datetime format\n",
    "df_sales['Date'] = pd.to_datetime(df_sales['Date'])\n",
    "\n",
    "# Extract date features\n",
    "df_sales['Year'] = df_sales['Date'].dt.year\n",
    "df_sales['Month'] = df_sales['Date'].dt.month\n",
    "df_sales['Day'] = df_sales['Date'].dt.day\n",
    "df_sales['DayOfWeek'] = df_sales['Date'].dt.dayofweek\n",
    "df_sales['Quarter'] = df_sales['Date'].dt.quarter\n",
    "df_sales['WeekOfYear'] = df_sales['Date'].dt.isocalendar().week\n",
    "\n",
    "# Create a flag for weekends\n",
    "df_sales['IsWeekend'] = df_sales['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Display the enhanced dataset\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Aggregate data to weekly level for time series forecasting\n",
    "df_weekly = df_sales.groupby(['Year', 'WeekOfYear', 'Product_ID', 'Store_ID'])[\n",
    "    'Sales_Quantity', 'Inventory_Level'\n",
    "].agg({\n",
    "    'Sales_Quantity': 'sum',\n",
    "    'Inventory_Level': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Create a date column for the week\n",
    "df_weekly['Week_Start'] = df_weekly.apply(\n",
    "    lambda row: pd.to_datetime(f\"{row['Year']}-W{row['WeekOfYear']:02d}-1\", format='%Y-W%W-%w'),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display the weekly aggregated data\n",
    "df_weekly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create lag features for each product-store combination\n",
    "def create_lag_features(group, lags=[1, 2, 3, 4]):\n",
    "    for lag in lags:\n",
    "        group[f'Sales_Lag_{lag}'] = group['Sales_Quantity'].shift(lag)\n",
    "    return group\n",
    "\n",
    "# Apply the function to each product-store group\n",
    "df_weekly_with_lags = df_weekly.sort_values(['Product_ID', 'Store_ID', 'Week_Start']).groupby(['Product_ID', 'Store_ID']).apply(create_lag_features).reset_index(drop=True)\n",
    "\n",
    "# Create rolling mean features\n",
    "def create_rolling_features(group, windows=[2, 4, 8]):\n",
    "    for window in windows:\n",
    "        group[f'Sales_Rolling_{window}'] = group['Sales_Quantity'].shift(1).rolling(window=window, min_periods=1).mean()\n",
    "    return group\n",
    "\n",
    "# Apply the function to each product-store group\n",
    "df_weekly_features = df_weekly_with_lags.sort_values(['Product_ID', 'Store_ID', 'Week_Start']).groupby(['Product_ID', 'Store_ID']).apply(create_rolling_features).reset_index(drop=True)\n",
    "\n",
    "# Drop rows with NaN values (first few weeks for each product-store combination)\n",
    "df_weekly_features = df_weekly_features.dropna()\n",
    "\n",
    "# Display the final dataset with features\n",
    "df_weekly_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the processed data\n",
    "df_weekly_features.to_csv('../data/processed_weekly_data.csv', index=False)\n",
    "print(\"Processed data saved to '../data/processed_weekly_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Splitting\n",
    "\n",
    "Split the data into training and testing sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sort by date\n",
    "df_weekly_features = df_weekly_features.sort_values('Week_Start')\n",
    "\n",
    "# Determine the split point (e.g., use the last 8 weeks for testing)\n",
    "split_date = df_weekly_features['Week_Start'].max() - pd.Timedelta(weeks=8)\n",
    "\n",
    "# Split the data\n",
    "train_data = df_weekly_features[df_weekly_features['Week_Start'] <= split_date]\n",
    "test_data = df_weekly_features[df_weekly_features['Week_Start'] > split_date]\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# Save the train and test datasets\n",
    "train_data.to_csv('../data/train_data.csv', index=False)\n",
    "test_data.to_csv('../data/test_data.csv', index=False)\n",
    "print(\"Train and test data saved to '../data/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "1. Loaded and explored the sales and inventory data\n",
    "2. Visualized key patterns and relationships\n",
    "3. Preprocessed the data for time series forecasting\n",
    "4. Created lag and rolling features\n",
    "5. Split the data into training and testing sets\n",
    "\n",
    "The processed data is now ready for model development in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
